import scrapy,json,os,platform,unicodedata
from crawldata.functions import *
from datetime import datetime

class CrawlerSpider(scrapy.Spider):
    name = 'hochrather'
    DATE_CRAWL=datetime.now().strftime('%Y-%m-%d')
    #custom_settings={'LOG_FILE':'./log/'+name+'_'+DATE_CRAWL+'.log'}
    if platform.system()=='Linux':
        URL='file:////' + os.getcwd()+'/scrapy.cfg'
    else:
        URL='file:///' + os.getcwd()+'/scrapy.cfg'
    domain='https://www.hochrather.at'
    url='https://www.hochrather.at/ersatzteile'
    URLS=[]
    def start_requests(self):
        self.URLS.append(key_MD5(self.url))
        yield scrapy.Request(self.url,callback=self.parse_cat,dont_filter=True,meta={'cats':[],'pos':0})
    def parse_cat(self,response):
        pos = response.meta.get('pos', 0)
        cats = response.meta.get('cats', [])
        
        Data=response.xpath('//a[contains(@class,"term-link")]/@href').getall()
        for link in Data:
            yield scrapy.Request(response.urljoin(link),callback=self.parse_cat,dont_filter=True,meta={'pos':pos})
        
        if not Data:
            breadcrumb = response.xpath('//div[@id="breadcrumb"]/a[position() > 2]/text() | //span[@class="curr-bradcrumb"]/text()').getall()

            Data=response.xpath('//ul[@class="products"]/li/a/@href').getall()
            for link in Data:
                pos+=1
                if key_MD5(link) not in self.URLS:
                    self.URLS.append(key_MD5(link))
                yield scrapy.Request(response.urljoin(link),callback=self.parse_data,dont_filter=True,meta={'cats':breadcrumb,'pos':pos})

            next_page=response.xpath('//a[@class="next page-numbers"]/@href').get()
            if next_page:
                cur_url=response.url
                if '/page/' in cur_url:
                    page_number = int(cur_url.split('/')[-1])
                else:
                    page_number = 1
                
                base_url = cur_url.split('/page/')[0]
                page_number += 1
                next_url = base_url + f"/page/{page_number}"
                if key_MD5(next_url) not in self.URLS:
                    self.URLS.append(key_MD5(next_url))
                yield scrapy.Request(next_url,callback=self.parse_cat,dont_filter=True,meta={'cats':breadcrumb,'pos':pos})

    def parse_data(self,response):
        cats=response.meta.get('cats', [])
        ID=response.xpath('//div[contains(@id,"product-")]/@id').get()
        if ID:
            cate=[]
            for i in range(len(cats)):
                if str(cats[i]).strip()!='':
                    cate.append(str(cats[i]).strip())
            print(response.url)
            print(cate)
            
            title=response.xpath('//h1/text()').get()
            sku=response.xpath('//p[@class="sku"]/text()').get()
            item={}
            item['oem_quality']=0
            original=response.xpath('//span[@class="original"]')
            if original:
                item['oem_quality']=1
            img=response.xpath('//img[contains(@class,"embed-responsive-item")]/@src').get()
            if img:
                item['base_image']=img
                item['small_image']=str(item['base_image']).replace("-large.",".")
                item['thumbnail_image']=item['base_image']
            item['part_number']=str(sku).strip().split()[-1]
            if cats:
                item['brand']=cats[0]
            else:
                item['brand']='unbranded'

            item['name']=str(title).strip()
            item['breadcrumb']=('/'.join(cats))
            item['original_page_url']=response.url
            item['sku']=item['brand']+'-'+item['part_number']
            item['qty']=response.xpath('//div[contains(@class,"entry-summary")]//input[@name="quantity"]/@value').get()
            # item['item_position']=response.meta['pos']
            item['original_id']=key_MD5(item['breadcrumb'])+'_'+ID

            item['price']=Get_Number(response.xpath('//div[@class="discount"]//bdi/text()').get())
            if not item['price']:
                item['price'] = 0.0

            item['discount_price']=Get_Number(response.xpath('//p[@class="price-with-discount"]//bdi/text()').get())
            if not item['discount_price']:
                item['discount_price'] = 0.0

            if not item['qty']:
                item['qty'] = 0

            currency=response.xpath('//div[@class="discount"]//span[@class="woocommerce-Price-currencySymbol"]/text()').get()
            if currency:
                if currency=='â‚¬':
                    currency='EUR'
                item['price_currency']=currency
            item['tech_spec']={}
            tecths=response.xpath('//div[contains(@id,"product-")]//div[contains(@class,"col-tech")]//table[@class="prop-table"]//tr')
            for rs in tecths:
                TXT=rs.xpath('./td[not(@title)]/text()').get()
                if TXT:
                    item['tech_spec'][rs.xpath('./td[@title]/@title').get()]=unicodedata.normalize("NFKD",rs.xpath('./td[not(@title)]/text()').get()).replace('\"', '')
            desc=[]
            spec=response.xpath('//div[contains(@id,"product-")]//div[contains(@class,"col-spec")]/div//text()').getall()
            for rs in spec:
                rs=str(rs).strip()
                if rs!='':
                    desc.append(rs)
            item['description']="; ".join(desc)
            headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:136.0) Gecko/20100101 Firefox/136.0','Accept': '*/*','Accept-Language': 'en-US,en;q=0.5','Referer': response.url,'Connection': 'keep-alive','Sec-Fetch-Dest': 'empty','Sec-Fetch-Mode': 'cors','Sec-Fetch-Site': 'same-origin','Priority': 'u=4'}
            url='https://www.hochrather.at/wp-json/product/v1/get-product-terms?product_id='+str(ID).replace("product-","")
            yield scrapy.Request(url,callback=self.parse_fit,headers=headers,meta={'item':item},dont_filter=True)
        else:
            print(response.url)
    def parse_fit(self,response):
        item=response.meta['item']
        Data=json.loads(response.text)
        if len(Data['data'])>0:
            item['equipment_fit']=Data['data']
        yield(item)
